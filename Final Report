We decided to find datasets through Kaggle, as the interface is very user friendly, and you can quickly download 
json files of large data sets. The datasets we chose came from "Police Violence and Racial Equity - Part 2 of 3", 
which included demographics, crime statistics, and other useful data in the form of csv files. To do this, we each 
created accounts on Kaggle, and requested an API token for access to the json zip downloads. This token was then 
saved onto our individual machines in a hidden Kaggle folder that we each then pulled into a jupyter notebook via 
terminal commands. Importing zipfile into our notebooks allowed us to access and quickly extract the zip files within, 
so that we could then use pandas to easily read in the csv's just by changing the index of each dataset list. We 
decided on four csv's out of the data set: 
1. national adults arrested data, <br>2. Chicago crime and arrest data, <br>
3. national police info/budget data, <br>4. NYPD crime and arrest data. 
 
Each of us did our own ETL process for each of the csv's listed above so that all of us could get more practice with 
the entire process of extract, transform, and load. The process for each of the csv's was similar, however we did 
certain steps slightly differently. Here are each of the steps for each csv dataframe we extracted, transformed, then loaded:

1. National adults arrested data (Carley Engleson)

 - Pull in csv, create dataframe
 - interpreted which columns were important after reading csv in jupyter notebook
 - cleaned table and removed unnecessary columns
 - created police-violence-racial-equity database in pgadmin
 - created adults_arrested table in pgadmin, added columns from cleaned df
 - created connection to postgresql database
 - appended df to adults_arrested table in pgadmin
 - confirmed transfer of data into database using pandas to read sql query

2. Chicago crime and arrest data (Cheyenne Martin)

 - Read the CSV into jupyter notebook
 - Printed the name of the columns since the file was large
 - Selected the appropriate columns that I was interested in: 'ID', 'Primary Type', 'Description', and 'Year'
 - Connected to SQL by creating an engine
 - Set up the engine table names
 - Pushed the Chicago Crime dataframe to SQL
 - Used pandas to check if the connection was viable
 

3. National police info/budget data (Derek Larson)

 - Used dropna to remove null values.
 - Grabbed desired columns.
 - Renamed columns.
 - Set index to city_name and year.
 - Used sqlalchemy to create an engine and session to a sqlite db.
 - Created blank table.
 - Inspected db to validate table exists.
 - Pushed the dataframe to the table and appended if it existed.
 - Executed a select query and created it into a pd.Dataframe to validate data was inside.
 
 4. Dallas and NYPD crime data (Sam Warren)
 
 - Read in csv file and stored in dataframe
 - Cleaned  dataframe and eliminated unwanted columns
 - Created connection to local database in PgAdmin
 - Checked tables
 - Confirmed data had been added to database by querying the Dallas table
